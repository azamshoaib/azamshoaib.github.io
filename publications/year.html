<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Shoaib  Azam</title>
    <meta name="author" content="Shoaib  Azam">
    <meta name="description" content="Shoaib Azam - personal website
">
    <meta name="keywords" content="autonomous driving, deep learning, generative ai, representation learning">
    <!-- OpenGraph -->
    <meta property="og:site_name" content="Shoaib  Azam">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Shoaib  Azam | Publications">
    <meta property="og:url" content="https://azamshoaib.github.io//publications/year">
    <meta property="og:description" content="Shoaib Azam - personal website
">
    <meta property="og:image" content="https://azamshoaib.github.io/assets/img/shoaib.png">
    <meta property="og:locale" content="en">

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Publications">
    <meta name="twitter:description" content="Shoaib Azam - personal website
">
    <meta name="twitter:image" content="https://azamshoaib.github.io/assets/img/shoaib.png">
    

    <!-- Schema.org -->
    <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Shoaib  Azam"
        },
        "url": "https://azamshoaib.github.io//publications/year",
        "@type": "WebSite",
        "description": "Shoaib Azam - personal website
",
        "headline": "Publications",
        "sameAs": ["https://orcid.org/0000-0003-3521-5098", "https://scholar.google.com/citations?user=7twUsBMAAAAJ", "https://github.com/azamshoaib", "https://www.linkedin.com/in/azamshoaib"],
        "name": "Shoaib  Azam",
        "@context": "https://schema.org"
    }
    </script>


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://azamshoaib.github.io//publications/year">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

    <script async defer src="https://buttons.github.io/buttons.js"></script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><i class="fas fa-home"></i> <span class="font-weight-bold">Shoaib </span>Azam</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/type">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">Vitae</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
<div class="post">

  <!-- <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header> -->

  <article>
    <header class="post-header">
    <h1 class="post-title">Publications</h1>
</header>

<div class="bib-header">
    <span style="display:block;" class="block">Publications ordered by <b><i>year</i></b> in reverse chronological order.</span>
    <span class="block">Order by:</span>
    <a href="/publications/type" class="fas fa-sort btn" role="button" title="Type"> type</a>
</div>

<div style="counter-reset:bibitem 30"></div>

<div class="publications">
    <h2 class="year">2024</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/eaai_2024.png"></div>

        <!-- Entry bib key -->
        <div id="azamexploring" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving</div>
          <!-- Author -->
          <div class="author">
            

            </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2210.06758,</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://arxiv.org/pdf/2210.06758.pdf" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Learning contextual and spatial environmental representations enhances autonomous vehicle’s hazard anticipation and decision-making in complex scenarios. Recent perception systems enhance spatial understanding with sensor
fusion but often lack full environmental context. Humans, when driving, naturally employ neural maps that integrate various factors such as historical
data, situational subtleties, and behavioral predictions of other road users
to form a rich contextual understanding of their surroundings. This neural
map-based comprehension is integral to making informed decisions on the
road. In contrast, even with their significant advancements, autonomous
systems have yet to fully harness this depth of human-like contextual understanding. Motivated by this, our work draws inspiration from human
driving patterns and seeks to formalize the sensor fusion approach within
an end-to-end autonomous driving framework. We introduce a framework
that integrates three cameras (left, right, and center) to emulate the human
field of view, coupled with top-down bird-eye-view semantic data to enhance
contextual representation. The sensor data is fused and encoded using a
self-attention mechanism, leading to an auto-regressive waypoint prediction
module. We treat feature representation as a sequential problem, employing a vision transformer to distill the contextual interplay between sensor
modalities. The efficacy of the proposed method is experimentally evaluated
in both open and closed-loop settings. Our method achieves displacement
error by 0.67m in open-loop settings, surpassing current methods by 6.9%
on the nuScenes dataset. In closed-loop evaluations on CARLA’s Town05
Long and Longest6 benchmarks, the proposed method enhances driving performance, route completion, and reduces infractions.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azamexploring</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2210.06758}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/aaai_2024.png"></div>

        <!-- Entry bib key -->
        <div id="kujanpaa2024challenges" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Challenges of Data-Driven Simulation of Diverse and Consistent Human Driving Behaviors</div>
          <!-- Author -->
          <div class="author">
            

            Kalle Kujanpää, Daulet Baimukashev, Shibei Zhu, <em>Shoaib Azam</em>, Farzeen Munir, Gokhan Alcan, and Ville Kyrki</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In </em>, 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://arxiv.org/abs/2401.03236" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Building simulation environments for developing and testing autonomous vehicles necessitates that the simulators accurately model the statistical realism of the real-world environment, including the interaction with other vehicles driven by human drivers. To address this requirement, an accurate human behavior model is essential to incorporate the diversity and consistency of human driving behavior. We propose a mathematical framework for designing a data-driven simulation model that simulates human driving behavior more realistically than the currently used physics-based simulation models. Experiments conducted using the NGSIM dataset validate our hypothesis regarding the necessity of considering the complexity, diversity, and consistency of human driving behavior when aiming to develop realistic simulators.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kujanpaa2024challenges</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Challenges of Data-Driven Simulation of Diverse and Consistent Human Driving Behaviors}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kujanp{\"a}{\"a}, Kalle and Baimukashev, Daulet and Zhu, Shibei and Azam, Shoaib and Munir, Farzeen and Alcan, Gokhan and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2401.03236}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/hri_2024.png"></div>

        <!-- Entry bib key -->
        <div id="10.1145/3610978.3640625" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Exploring Large Language Models for Trajectory Prediction: A Technical Perspective</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, Tsvetomila Mihaylova, <em>Shoaib Azam</em>, Tomasz Piotr Kucner, and Ville Kyrki</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</em>, 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI" class="ai ai-doi btn z-depth-0" role="button"></a>
             -->
          
            <a href="https://dl.acm.org/doi/pdf/10.1145/3610978.3640625" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Large Language Models (LLMs) have been recently proposed for trajectory prediction in autonomous driving, where they potentially can provide explainable reasoning capability about driving situations. Most studies use versions of the OpenAI GPT, while there are open-source alternatives which have not been evaluated in this context. In this report1, we study their trajectory prediction performance as well as their ability to reason about the situation. Our results indicate that open-source alternatives are feasible for trajectory prediction. However, their ability to describe situations and reason about potential consequences of actions appears limited, and warrants future research.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3610978.3640625</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Mihaylova, Tsvetomila and Azam, Shoaib and Kucner, Tomasz Piotr and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring Large Language Models for Trajectory Prediction: A Technical Perspective}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703232}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3610978.3640625}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3610978.3640625}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{774–778}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{autonomous driving, large language models, trajectory prediction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{&lt;conf-loc&gt;, &lt;city&gt;Boulder&lt;/city&gt;, &lt;state&gt;CO&lt;/state&gt;, &lt;country&gt;USA&lt;/country&gt;, &lt;/conf-loc&gt;}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{HRI '24}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2023</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/drfuser.png"></div>

        <!-- Entry bib key -->
        <div id="munir2023multimodal" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Multimodal fusion for sensorimotor control in steering angle prediction</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Kin-Choong Yow, Byung-Geun Lee, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Engineering Applications of Artificial Intelligence,</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.sciencedirect.com/science/article/pii/S095219762301271X" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Imitation learning is employed to learn sensorimotor coordination for steering angle prediction in an end-to-end fashion requires expert demonstrations. These expert demonstrations are paired with environmental perception and vehicle control data. The conventional frame-based RGB camera is the most common exteroceptive sensor modality used to acquire the environmental perception data. The frame-based RGB camera has produced promising results when used as a single modality in learning end-to-end lateral control. However, the conventional frame-based RGB camera has limited operability in illumination variation conditions and is affected by the motion blur. The event camera provides complementary information to the frame-based RGB camera. This work explores the fusion of frame-based RGB and event data for learning end-to-end lateral control by predicting steering angle. In addition, how the representation from event data fuse with frame-based RGB data helps to predict the lateral control robustly for the autonomous vehicle. To this end, we propose DRFuser, a novel convolutional encoder-decoder architecture for learning end-to-end lateral control. The encoder module is branched between the frame-based RGB data and event data along with the self-attention layers. Moreover, this study has also contributed to our own collected dataset comprised of event, frame-based RGB, and vehicle control data. The efficacy of the proposed method is experimentally evaluated on our collected dataset, Davis Driving dataset (DDD), and Carla Eventscape dataset. The experimental results illustrate that the proposed method DRFuser outperforms the state-of-the-art in terms of root-mean-square error (RMSE) and mean absolute error (MAE) used as evaluation metrics.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">munir2023multimodal</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodal fusion for sensorimotor control in steering angle prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Yow, Kin-Choong and Lee, Byung-Geun and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Engineering Applications of Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{126}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{107087}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/itsc_2023.png"></div>

        <!-- Entry bib key -->
        <div id="munir2023radar" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Tomasz Kucner, Ville Kyrkil, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</em>, 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10422295" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn’t fully capture the environment due to the data’s sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by 1.89% and 2.61% in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2023radar</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Kucner, Tomasz and Kyrkil, Ville and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3575--3582}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2022</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/asc_2022.png"></div>

        <!-- Entry bib key -->
        <div id="munir2022exploring" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Exploring thermal images for object detection in underexposure regions for autonomous driving</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Muhammd Aasim Rafique, Ahmad Muqeem Sheri, Moongu Jeon, and Witold Pedrycz</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Applied Soft Computing,</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.sciencedirect.com/science/article/pii/S1568494622002125?casa_token=dU9Sk37lIw0AAAAA:lcxIDjjUtQF632jB2JNJoxBhn-cPSgyC1fa9MRlVWWMNU4VjB8U8LQObTc6RvUxe_0vFRy6%E2%80%93Q27" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Underexposure regions are vital in constructing a complete perception of the surrounding environment for safe autonomous driving. The availability of thermal cameras has provided an essential alternative to explore regions where other optical sensors lack in capturing interpretable signals. A thermal camera captures an image using the heat difference emitted by objects in the infrared spectrum, and object detection in thermal images becomes effective for autonomous driving in challenging conditions. Although object detection in the visible spectrum domain has matured, thermal object detection lacks effectiveness. A significant challenge is the scarcity of labeled data for the thermal domain, which is essential for SOTA artificial intelligence techniques. This work proposes a domain adaptation framework that employs a style transfer technique for transfer learning from visible spectrum images to thermal images. The framework uses a generative adversarial network (GAN) to transfer the low-level features from the visible spectrum domain to the thermal domain through style consistency. The efficacy of the proposed object detection method in thermal images is evident from the improved results when using styled images from publicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral).</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">munir2022exploring</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring thermal images for object detection in underexposure regions for autonomous driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Rafique, Muhammd Aasim and Sheri, Ahmad Muqeem and Jeon, Moongu and Pedrycz, Witold}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Soft Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{121}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{108793}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/tvt_2022.png"></div>

        <!-- Entry bib key -->
        <div id="hussain2022drivable" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Drivable region estimation for self-driving vehicles using radar</div>
          <!-- Author -->
          <div class="author">
            

            Muhammad Ishfaq Hussain, <em>Shoaib Azam</em>, Muhammad Aasim Rafique, Ahmad Muqeem Sheri, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Vehicular Technology,</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9740418" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Self-driving vehicles are posing new challenges as the automation level defined in the SAE International standards for autonomous driving is increased. A pivotal task in autonomous driving is building a perception of the surrounding environment using optical sensors, which is a long-standing challenge and prompts us to explore the utilization of various sensors. Radar is an older and cheaper type of sensor than alternatives such as lidar for long-range distance coverage, and it is also competitively reliable and robust in adverse weather conditions. However, sparse data and noise are inherent challenges of radar. This study explores the dynamic Gaussian process for occupancy mapping and predicting a drivable path for a self-driving vehicle within the field of view (FOV) of a radar sensor. Gaussian occupancy mapping does not need abundant data for training and is a promising alternative to data-reliant deep learning techniques. The proposed technique optimizes parameters (variational and kernel-based) of the Gaussian process to determine the allowed region within the FOV limits by means of stochastic selection of functional points (pseudoinput) and tuning of threshold values. We have tested the proposed technique in experiments performed under different environmental conditions, such as various road and traffic conditions and diverse weather and illumination conditions. The results verify the efficacy of the proposed technique in diverse weather conditions for finding a drivable path for a self-driving vehicle, with the additional benefits of requiring only a low-cost apparatus and providing coverage of a long distance range.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2022drivable</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Drivable region estimation for self-driving vehicles using radar}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Muhammad Ishfaq and Azam, Shoaib and Rafique, Muhammad Aasim and Sheri, Ahmad Muqeem and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Vehicular Technology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{71}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5971--5982}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2021</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/n2c_2021.png"></div>

        <!-- Entry bib key -->
        <div id="azam2021n" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">N 2 C: neural network controller design using behavioral cloning</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, Muhammad Aasim Rafique, Ahmad Muqeem Sheri, Muhammad Ishfaq Hussain, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Intelligent Transportation Systems,</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9312433" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Modern vehicles communicate data to and from sensors, actuators, and electronic control units (ECUs) using Controller Area Network (CAN) bus, which operates on differential signaling. An autonomous ECU responsible for the execution of decision commands to an autonomous vehicle is developed by assimilating the information from the CAN bus. The conventional way of parsing the decision commands is motion planning, which uses a path tracking algorithm to evaluate the decision commands. This study focuses on designing a robust controller using behavioral cloning and motion planning of autonomous vehicle using a deep learning framework. In the first part of this study, we explore the pipeline of parsing decision commands from the path tracking algorithm to the controller and proposed a neural network-based controller ( N 2 C) using behavioral cloning. The proposed network predicts throttle, brake, and torque when trained with the manual driving data acquired from the CAN bus. The efficacy of the proposed method is demonstrated by comparing the accuracy with the Proportional-Derivative-Integral (PID) controller in conjunction with the path tracking algorithm (pure pursuit and model predictive control based path follower). The second part of this study complements N 2 C, in which an end-to-end neural network for predicting the speed and steering angle is proposed with image data as an input. The performance of the proposed frameworks are evaluated in real-time and on the Udacity dataset, showing better metric scores in the former and reliable prediction in the later case when compared with the state-of-the-art methods.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azam2021n</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{N 2 C: neural network controller design using behavioral cloning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Rafique, Muhammad Aasim and Sheri, Ahmad Muqeem and Hussain, Muhammad Ishfaq and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Transportation Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4744--4756}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/iv_2021.png"></div>

        <!-- Entry bib key -->
        <div id="azam2021channel" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Channel boosting feature ensemble for radar-based object detection</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2021 IEEE Intelligent Vehicles Symposium (IV)</em>, 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9575600" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Autonomous vehicles are conceived to provide safe and secure services by validating the safety standards as indicated by SOTIF-ISO/PAS-21448 (Safety of the intended functionality). Keeping in this context, the perception of the environment plays an instrumental role in conjunction with localization, planning and control modules. As a pivotal algorithm in the perception stack, object detection provides extensive insights into the autonomous vehicle’s surroundings. Camera and Lidar are extensively utilized for object detection among different sensor modalities, but these exteroceptive sensors have limitations in resolution and adverse weather conditions. In this work, radar-based object detection is explored provides a counterpart sensor modality to be deployed and used in adverse weather conditions. The radar gives complex data; for this purpose, a channel boosting feature ensemble method with transformer encoder-decoder network is proposed. The object detection task using radar is formulated as a set prediction problem and evaluated on the publicly available dataset [1] in both good and good-bad weather conditions. The proposed method’s efficacy is extensively evaluated using the COCO evaluation metric, and the best-proposed model surpasses its state-of-the-art counterpart method by 12.55% and 12.48% in both good and good-bad weather conditions.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2021channel</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Channel boosting feature ensemble for radar-based object detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE Intelligent Vehicles Symposium (IV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{762--769}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/iros_2021.png"></div>

        <!-- Entry bib key -->
        <div id="munir2021sstn" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Sstn: Self-supervised domain adaptation thermal object detection for autonomous driving</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2021 IEEE/RSJ international conference on intelligent robots and systems (IROS)</em>, 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9636353" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The perception of the environment plays a decisive role in the safe and secure operation of autonomous vehicles. The perception of the surrounding is way similar to human vision. The human’s brain perceives the environment by utilizing different sensory channels and develop a view-invariant representation model. In this context, different exteroceptive sensors like cameras, Lidar, are deployed on the autonomous vehicle to perceive the environment. These sensors have illustrated their benefit in the visible spectrum domain yet in the adverse weather conditions; for instance, they have limited operational capability at night, leading to fatal accidents. This work explores thermal object detection to model a view-invariant model representation by employing the self-supervised contrastive learning approach. We have proposed a deep neural network Self Supervised Thermal Network (SSTN) for learning the feature embedding to maximize the information between visible and infrared spectrum domain by contrastive learning. Later, these learned feature representations are employed for thermal object detection using a multi-scale encoder-decoder transformer network. The proposed method is extensively evaluated on the two publicly available datasets: the FLIR-ADAS dataset and the KAIST Multi-Spectral dataset. The experimental results illustrate the efficacy of the proposed method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2021sstn</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sstn: Self-supervised domain adaptation thermal object detection for autonomous driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE/RSJ international conference on intelligent robots and systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{206--213}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/its_key_2021.png"></div>

        <!-- Entry bib key -->
        <div id="ko2021key" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Key points estimation and point instance segmentation approach for lane detection</div>
          <!-- Author -->
          <div class="author">
            

            Yeongmin Ko, Younkwan Lee, <em>Shoaib Azam</em>, Farzeen Munir, Moongu Jeon, and Witold Pedrycz</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Intelligent Transportation Systems,</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9460822" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Perception techniques for autonomous driving should be adaptive to various environments. In essential perception modules for traffic line detection, many conditions should be considered, such as a number of traffic lines and computing power of the target system. To address these problems, in this paper, we propose a traffic line detection method called Point Instance Network (PINet); the method is based on the key points estimation and instance segmentation approach. The PINet includes several hourglass models that are trained simultaneously with the same loss function. Therefore, the size of the trained models can be chosen according to the target environment’s computing power. We cast a clustering problem of the predicted key points as an instance segmentation problem; the PINet can be trained regardless of the number of the traffic lines. The PINet achieves competitive accuracy and false positive on CULane and TuSimple datasets, popular public datasets for lane detection. Our code is available at https://github.com/koyeongmin/PINet_new</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ko2021key</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Key points estimation and point instance segmentation approach for lane detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ko, Yeongmin and Lee, Younkwan and Azam, Shoaib and Munir, Farzeen and Jeon, Moongu and Pedrycz, Witold}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Transportation Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8949--8958}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/its_ldnet_2021.png"></div>

        <!-- Entry bib key -->
        <div id="munir2021ldnet" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">LDNet: End-to-end lane marking detection approach using a dynamic vision sensor</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Moongu Jeon, Byung-Geun Lee, and Witold Pedrycz</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Intelligent Transportation Systems,</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9518365" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Modern vehicles are equipped with various driver-assistance systems, including automatic lane keeping, which prevents unintended lane departures. Traditional lane detection methods incorporate handcrafted or deep learning-based features followed by postprocessing techniques for lane extraction using frame-based RGB cameras. The utilization of frame-based RGB cameras for lane detection tasks is prone to illumination variations, sun glare, and motion blur, which limits the performance of lane detection methods. Incorporating an event camera for lane detection tasks in the perception stack of autonomous driving is one of the most promising solutions for mitigating challenges encountered by frame-based RGB cameras. The main contribution of this work is the design of the lane marking detection model, which employs the dynamic vision sensor. This paper explores the novel application of lane marking detection using an event camera by designing a convolutional encoder followed by the attention-guided decoder. The spatial resolution of the encoded features is retained by a dense atrous spatial pyramid pooling (ASPP) block. The additive attention mechanism in the decoder improves performance for high dimensional input encoded features that promote lane localization and relieve postprocessing computation. The efficacy of the proposed work is evaluated using the DVS dataset for lane extraction (DET). The experimental results show a significant improvement of 5.54% and 5.03% in F1 scores in multiclass and binary-class lane marking detection tasks. Additionally, the intersection over union ( IoU ) scores of the proposed method surpass those of the best-performing state-of-the-art method by 6.50% and 9.37% in multiclass and binary-class tasks, respectively.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">munir2021ldnet</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LDNet: End-to-end lane marking detection approach using a dynamic vision sensor}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Jeon, Moongu and Lee, Byung-Geun and Pedrycz, Witold}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Transportation Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9318--9334}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/acpr_2021.png"></div>

        <!-- Entry bib key -->
        <div id="munir2021artseg" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">ARTSEG: Employing attention for thermal images semantic segmentation</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Unse Fatima, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Asian Conference on Pattern Recognition</em>, 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-02375-0_27" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The research advancements have made the neural network algorithms deployed in the autonomous vehicle to perceive the surrounding. The standard exteroceptive sensors that are utilized for the perception of the environment are cameras and Lidar. Therefore, the neural network algorithms developed using these exteroceptive sensors have provided the necessary solution for the autonomous vehicle’s perception. One major drawback of these exteroceptive sensors is their operability in adverse weather conditions, for instance, low illumination and night conditions. The useability and affordability of thermal cameras in the sensor suite of the autonomous vehicle provide the necessary improvement in the autonomous vehicle’s perception in adverse weather conditions. The semantics of the environment benefits the robust perception, which can be achieved by segmenting different objects in the scene. In this work, we have employed the thermal camera for semantic segmentation. We have designed an attention-based Recurrent Convolution Network (RCNN) encoder-decoder architecture named ARTSeg for thermal semantic segmentation. The main contribution of this work is the design of encoder-decoder architecture, which employ units of RCNN for each encoder and decoder block. Furthermore, additive attention is employed in the decoder module to retain high-resolution features and improve the localization of features. The efficacy of the proposed method is evaluated on the available public dataset, showing better performance with other state-of-the-art methods in mean intersection over union (IoU).</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2021artseg</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ARTSEG: Employing attention for thermal images semantic segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Fatima, Unse and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Asian Conference on Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{366--378}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer International Publishing Cham}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2020</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/info_sci_2020.png"></div>

        <!-- Entry bib key -->
        <div id="dinh2020transfer" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Transfer learning for vehicle detection using two cameras with different focal lengths</div>
          <!-- Author -->
          <div class="author">
            

            Vinh Quang Dinh, Farzeen Munir, <em>Shoaib Azam</em>, Kin-Choong Yow, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Information Sciences,</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.sciencedirect.com/science/article/pii/S0020025519310795?casa_token=qj1jl3kRigMAAAAA:HIlY0ie2yJUKee0PIDUwRUoWONIswqcLlLpLF9YfFvtGbHXaTdT8NqF0o4hzE_X7dhnDchf_hAsY" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a vehicle detection method using transfer learning for two cameras with different focal lengths. A detected vehicle region in an image of one camera is transformed into a binary map. After that, the map is used to filter convolutional neural network (CNN) feature maps which are computed for the other camera’s image. We also introduce a robust evolutionary algorithm that is used to compute the relationship between the two cameras in an off-line mode efficiently. We capture video sequences and sample them to make a dataset that includes images with different focal lengths for vehicle detection. We compare the proposed vehicle detection method with baseline detection methods, including faster region proposal networks (Faster-RCNN), single-shot-multi-Box detector (SSD), and detector using recurrent rolling convolution (RRC), in the same experimental context. The experimental results show that the proposed method can detect vehicles at a wide range of distances accurately and robustly, and significantly outperforms the baseline detection methods.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dinh2020transfer</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transfer learning for vehicle detection using two cameras with different focal lengths}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dinh, Vinh Quang and Munir, Farzeen and Azam, Shoaib and Yow, Kin-Choong and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Information Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{514}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{71--87}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/vehits_2020.png"></div>

        <!-- Entry bib key -->
        <div id="azam2020dynamic" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Dynamic Control System Design for Autonomous Car</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In VEHITS</em>, 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.scitepress.org/Papers/2020/93929/93929.pdf" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The autonomous vehicle requires higher standards of safety to maneuver in a complex environment. We focus on control of the self-driving vehicle that includes the longitudinal and lateral dynamics of the vehicle. In this work, we have developed a customized controller for our KIA Soul self-driving car. The customized controller implements the PID control for throttle, brake, and steering so that the vehicle follows the desired velocity profile, which enables a comfortable and safe ride. Besides, we have also catered the lateral dynamic model with two approaches: pure pursuit and model predictive control. An extensive analysis is performed between pure pursuit and its adversary model predictive control for the efficacy of the lateral model.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2020dynamic</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Control System Design for Autonomous Car}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{VEHITS}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{456--463}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/iemtronics_2020.png"></div>

        <!-- Entry bib key -->
        <div id="hussain2020multiple" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Multiple objects tracking using radar for autonomous driving</div>
          <!-- Author -->
          <div class="author">
            

            Muhamamd Ishfaq Hussain, <em>Shoaib Azam</em>, Farzeen Munir, Zafran Khan, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)</em>, 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9216363" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Object detection and tracking are the integral elements for the perception of the spatio-temporal environment. The availability and affordability of camera and lidar as the leading sensor modalities have used for object detection and tracking in research. The usage of deep learning algorithms for the object detection and tracking using camera and lidar have illustrated the promising results, but these sensor modalities are prone to weather conditions, have sparse data and spatial resolution problems. In this work, we explore the problem of detecting distant objects and tracking using radar. For the efficacy of our proposed work, extensive experimentation in different traffic scenario are performed by using our self-driving car test-bed.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2020multiple</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multiple objects tracking using radar for autonomous driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Muhamamd Ishfaq and Azam, Shoaib and Munir, Farzeen and Khan, Zafran and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--4}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/archi.png"></div>

        <!-- Entry bib key -->
        <div id="azam2020system" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">System, design and experimental validation of autonomous vehicle in an unconstrained environment</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, Ahmad Muqeem Sheri, Joonmo Kim, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Sensors,</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.mdpi.com/1424-8220/20/21/5999" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In recent years, technological advancements have made a promising impact on the development of autonomous vehicles. The evolution of electric vehicles, development of state-of-the-art sensors, and advances in artificial intelligence have provided necessary tools for the academia and industry to develop the prototypes of autonomous vehicles that enhance the road safety and traffic efficiency. The increase in the deployment of sensors for the autonomous vehicle, make it less cost-effective to be utilized by the consumer. This work focuses on the development of full-stack autonomous vehicle using the limited amount of sensors suite. The architecture aspect of the autonomous vehicle is categorized into four layers that include sensor layer, perception layer, planning layer and control layer. In the sensor layer, the integration of exteroceptive and proprioceptive sensors on the autonomous vehicle are presented. The perception of the environment in term localization and detection using exteroceptive sensors are included in the perception layer. In the planning layer, algorithms for mission and motion planning are illustrated by incorporating the route information, velocity replanning and obstacle avoidance. The control layer constitutes lateral and longitudinal control for the autonomous vehicle. For the verification of the proposed system, the autonomous vehicle is tested in an unconstrained environment. The experimentation results show the efficacy of each module, including localization, object detection, mission and motion planning, obstacle avoidance, velocity replanning, lateral and longitudinal control. Further, in order to demonstrate the experimental validation and the application aspect of the autonomous vehicle, the proposed system is tested as an autonomous taxi service.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azam2020system</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{System, design and experimental validation of autonomous vehicle in an unconstrained environment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Sheri, Ahmad Muqeem and Kim, Joonmo and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5999}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/icce_asia_2020.png"></div>

        <!-- Entry bib key -->
        <div id="munir2020visuomotor" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Visuomotor Steering angle Prediction in Dynamic Perception Environment for Autonomous Vehicle</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2020 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)</em>, 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9276907" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>
Visuomotor coordination in driving assists the driver in performing necessary action expeditiously. Over the recent years, the development towards autonomous vehicles has accelerated tremendously, and an increase in the computational capabilities motivates us to train the complex deep neural network models using visual cues for better understanding of visuomotor coordination in the autonomous vehicles. In this work, we exploit this problem by using visual cues in the images to predict the steering angle for a self-driving car. The future frames are predicted to estimate a look-ahead steering angle so that self-driving car can make appropriate decisions. In our dynamic predictive model, we have used the deep convolution-LSTM model for predicting the future frames and a series of ResNet blocks for predicting the steering angle for the self-driving car. Moreover, we have used segmentation as an auxiliary information for training the dynamic predictive network. The efficacy of our dynamic predictive model is rigorously tested on our collected and Udacity dataset.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2020visuomotor</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visuomotor Steering angle Prediction in Dynamic Perception Environment for Autonomous Vehicle}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2019</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/optics_2019.png"></div>

        <!-- Entry bib key -->
        <div id="azam2019data" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Data fusion of lidar and thermal camera for autonomous driving</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, Ahmad Muqeem Sheri, YeongMin Ko, Ishfaq Hussain, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Applied Industrial Optics: Spectroscopy, Imaging and Metrology</em>, 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://drive.google.com/file/d/1Cnkt-_JQ06XT13B1RSgZVF9-VGGY-oUH/view?usp=sharing" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The adverse environmental conditions build a bottleneck for the autonomous driving. This challenge is resolved by data fusion of sensor modalities. Here, thermal and Lidar data are fused together for the precise perception of environment.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2019data</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data fusion of lidar and thermal camera for autonomous driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Sheri, Ahmad Muqeem and Ko, YeongMin and Hussain, Ishfaq and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Applied Industrial Optics: Spectroscopy, Imaging and Metrology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{T2A--5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Optica Publishing Group}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/icspcs_2019.png"></div>

        <!-- Entry bib key -->
        <div id="van2019automated" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Automated Taxi Booking Operations for Autonomous Vehicles</div>
          <!-- Author -->
          <div class="author">
            

            Linh Van Ma, <em>Shoaib Azam</em>, Farzeen Munir, Moongu Jeon, and Jinho Choi</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2019 13th International Conference on Signal Processing and Communication Systems (ICSPCS)</em>, 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9008449" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In a conventional taxi booking system, all taxi operations are mostly done by a decision made by drivers which is hard to implement in unmanned vehicles. To address this challenge, we introduce a taxi booking system which assists autonomous vehicles to pick up customers. The system can allocate an autonomous vehicle (AV) as well as plan service trips for a customer request. We use our own AV to serve a customer who uses a mobile application to make his taxi request. Apart from customer and AV, we build a server to monitor customers and AVs. It also supports inter-communication between a customer and an AV once AV decided to pick up a customer.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">van2019automated</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Taxi Booking Operations for Autonomous Vehicles}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Van Ma, Linh and Azam, Shoaib and Munir, Farzeen and Jeon, Moongu and Choi, Jinho}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 13th International Conference on Signal Processing and Communication Systems (ICSPCS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/vehtis_2019.png"></div>

        <!-- Entry bib key -->
        <div id="munir2019localization" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Where Am I: Localization and 3D Maps for Autonomous Vehicles</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Ahmad Muqeem Sheri, YeongMin Ko, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In VEHITS</em>, 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://pdfs.semanticscholar.org/be59/8f4f0d42d854c5c9541e9e72daf0dfbfec75.pdf" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The nuts and bolts of autonomous driving find its root in devising the localization strategy. Lidar as one of the newest technologies developed in the recent years, provides rich information about the environment in the form of point cloud data which can be used for localization. In this paper, we discuss a localization approach which generates a 3D map from Lidar’s point cloud data using Normal Distribution Transform (NDT) mapping. We use our own dataset collected using our self driving car KIA Soul EV equipped with Lidar and cameras. Once the 3D map has been generated, we have used NDT matching for localizing the self driving car.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2019localization</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Where Am I: Localization and 3D Maps for Autonomous Vehicles}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Sheri, Ahmad Muqeem and Ko, YeongMin and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{VEHITS}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{452--457}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2018</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/iv_2018.png"></div>

        <!-- Entry bib key -->
        <div id="azam2018object" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Object modeling from 3d point cloud data for self-driving vehicles</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Farzeen Munir, Aasim Rafique, YeongMin Ko, Ahmad Muqeem Sheri, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2018 IEEE Intelligent Vehicles Symposium (IV)</em>, 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8500500" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>For autonomous vehicles to be deployed and used practically, many problems are still needed to be solved. One of them we are interested in is to make use of a cheap LIDAR for robust object modelling with 3D point cloud data. Self-driving vehicles require accurate information about the surrounding environments to decide the next course of actions. 3D point cloud data obtained from LIDAR give more accurate distance than the counterpart stereo images. As LIDAR generates lowresolution data, the object detection and modeling is prone to produce errors. In this work, we propose the use of multiple frames of LIDAR data in an urban environment to construct a comprehensive model of the object. We assume the use of LIDAR on a moving platform and the results are almost equal to the 3D CAD model representation of the object.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2018object</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object modeling from 3d point cloud data for self-driving vehicles}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Munir, Farzeen and Rafique, Aasim and Ko, YeongMin and Sheri, Ahmad Muqeem and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2018 IEEE Intelligent Vehicles Symposium (IV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{409--414}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/ssip_2018.png"></div>

        <!-- Entry bib key -->
        <div id="munir2018autonomous" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Autonomous vehicle: The architecture aspect of self driving car</div>
          <!-- Author -->
          <div class="author">
            

            Farzeen Munir, <em>Shoaib Azam</em>, Muhammad Ishfaq Hussain, Ahmed Muqeem Sheri, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2018 International Conference on Sensors, Signal and Image Processing</em>, 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://dl.acm.org/doi/pdf/10.1145/3290589.3290599" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Self-driving cars have received a lot of attention in recent years and many stakeholders like Google, Uber, Tesla, and so forth have invested a lot in this area and developed their own autonomous driving car platforms. The challenge to make an autonomous car is not only the stringent performance but also the safety of the passengers and pedestrians. Even with the development of technologies, autonomous driving is still an active research area and still requires a lot of experimentations and making architecture entirely autonomous.
The intriguing area of self-driving car motivates us to build an autonomous driving platform. In this paper, we discuss the architecture of the self-driving car and its software components that include localization, detection, motion planning and mission planning. We also highlight the hardware modules that are responsible for controlling the car. The autonomous driving is running state-of-the-art algorithms used in localization, detection, mission and motion planning.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">munir2018autonomous</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Autonomous vehicle: The architecture aspect of self driving car}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Munir, Farzeen and Azam, Shoaib and Hussain, Muhammad Ishfaq and Sheri, Ahmed Muqeem and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 International Conference on Sensors, Signal and Image Processing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2017</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/patent.png"></div>

        <!-- Entry bib key -->
        <div id="rafique2017" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Face de-identification method (translated), Registration #: 10-1861520</div>
          <!-- Author -->
          <div class="author">
            

            Muhammad Aasim Rafique, <em>Shoaib Azam</em>, InMoon Choi, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://drive.google.com/file/d/1iMZ0UnI9gq4eOIY3vPbOo8hXHAfuuwIt/view?usp=drive_link" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p></p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">rafique2017</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Face de-identification method (translated), Registration #: 10-1861520}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rafique, Muhammad Aasim and Azam, Shoaib and Choi, InMoon and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>
  </div>

<div class="publications">
    <h2 class="year">2016</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/visapp_2016.png"></div>

        <!-- Entry bib key -->
        <div id="azam2016benchmark" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">A Benchmark of Computational Models of Saliency to Predict Human Fixations in Videos.</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Syed Omer Gilani, Moongu Jeon, Rehan Yousaf, and Jeong-Bae Kim</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In VISIGRAPP (4: VISAPP)</em>, 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://www.scitepress.org/papers/2016/56787/56787.pdf" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In many applications of computer graphics and design, robotics and computer vision, there is always a need
to predict where human looks in the scene. However this is still a challenging task that how human visual
system certainly works. A number of computational models have been designed using different approaches
to estimate the human visual system. Most of these models have been tested on images and performance is
calculated on this basis. A benchmark is made using images to see the immediate comparison between the
models. Apart from that there is no benchmark on videos, to alleviate this problem we have a created a
benchmark of six computational models implemented on 12 videos which have been viewed by 15 observers
in a free viewing task. Further a weighted theory (both manual and automatic) is designed and implemented
on videos using these six models which improved Area under the ROC. We have found that Graph Based
Visual Saliency (GBVS) and Random Centre Surround Models have outperformed the other models.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2016benchmark</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Benchmark of Computational Models of Saliency to Predict Human Fixations in Videos.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Gilani, Syed Omer and Jeon, Moongu and Yousaf, Rehan and Kim, Jeong-Bae}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{VISIGRAPP (4: VISAPP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{134--142}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/iccais_2016.png"></div>

        <!-- Entry bib key -->
        <div id="azam2016vehicle" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Vehicle pose detection using region based convolutional neural network</div>
          <!-- Author -->
          <div class="author">
            

            <em>Shoaib Azam</em>, Aasim Rafique, and Moongu Jeon</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2016 International Conference on Control, Automation and Information Sciences (ICCAIS)</em>, 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7822459" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In recent years, category-level object detection has gained a lot of attention. In addition to object localization, estimation of the object pose has practical applications in intelligent transportation, autonomous driving and robotics. Parts based models have been used for pose estimation in recent years, but these models depend on manual supervision or require a complex algorithm to locate the object parts. In this work, we have used Convolutional Neural Network for the pose estimation of vehicle in an image. The advantage of multiple classifications of objects at the same time motivates us to choose the convolutional neural network. We make use of state-of-the-art implementation of convolution neural network named the Region Based Convolutional Neural Network(FASTER-RCNN) for estimating the pose of vehicle. We annotate the comprehensive cars dataset of Stanford, required for training the model and upon testing we have achieved good results with good accuracy.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azam2016vehicle</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vehicle pose detection using region based convolutional neural network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Shoaib and Rafique, Aasim and Jeon, Moongu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 International Conference on Control, Automation and Information Sciences (ICCAIS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{194--198}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/icitst_2016.png"></div>

        <!-- Entry bib key -->
        <div id="rafique2016face" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Face-deidentification in images using restricted boltzmann machines</div>
          <!-- Author -->
          <div class="author">
            

            M Aasim Rafique, M Shoaib Azam, Moongu Jeon, and Sangwook Lee</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)</em>, 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7856669" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work, we discuss utility of Restricted Boltzmann Machine (RBM) in face-deidentification challenge. GRBM is a generative modeling technique and its unsupervised learning provides vantage of using raw faces data. Faces are deidentified by reconstructed face images from the trained GRBM model. The reconstructed image uses random information from the stochastic units which makes it hard to re-identify from the deidentified face. Experiments show the proposed technique maintain emotions in the test face, which is intrinsic to the modeling capacity of RBM.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rafique2016face</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Face-deidentification in images using restricted boltzmann machines}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rafique, M Aasim and Azam, M Shoaib and Jeon, Moongu and Lee, Sangwook}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{69--73}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/icce_asia_2016.png"></div>

        <!-- Entry bib key -->
        <div id="tahir2016single" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Single object tracking system using fast compressive tracking</div>
          <!-- Author -->
          <div class="author">
            

            Abdullah Tahir, <em>Shoaib Azam</em>, Sujani Sagabala, Moongu Jeon, and Ryu Jeha</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2016 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)</em>, 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7804760" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work we focused on the application aspect of object tracking for pan-tilt-zoom (PTZ) camera using ordinary webcam mounted on custom made motor-assembly and found that our system is not only robust to illumination conditions but also cost-effective in comparison with PTZ cameras. For object tracking we utilized Fast Compressive Tracking (FCT) algorithm because of its attractive features e.g. online learning, fast computation and robust performance. A PC program interfaced with embedded system through serial RS232 commands motors, hence camera, to real time track desired object in world such that object being tracked remains in the center of image.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tahir2016single</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Single object tracking system using fast compressive tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tahir, Abdullah and Azam, Shoaib and Sagabala, Sujani and Jeon, Moongu and Jeha, Ryu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--3}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
  </div>

<div class="publications">
    <h2 class="year">2015</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/adv_sci_2015.png"></div>

        <!-- Entry bib key -->
        <div id="azam2015saliency" class="col-sm-10">
        
          <!-- Title -->
          <div style="font-size: 20px;" class="title">Saliency Based Object Detection and Enhancements Using Spectral Residual Approach in Static Images and Videos</div>
          <!-- Author -->
          <div class="author">
            

            Muhammad Shoaib Azam, Syed Omer Gilani, Mohsin Jamil, Yasar Ayaz, Muhammad Naveed, and Muhammad Nasir Khan</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advanced Science Letters,</em> 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links align-items-center  btn-group btn-group-sm" role="group">
            <a class="abstract fa fa-font btn z-depth-0" title="Show Abstract" role="button"></a>
          
            <a title="Bibtex (Show/hide)" class="bibtex fa fa-book btn z-depth-0" role="button"></a>
          
          

          
            <!-- 
              <a href=""  title="DOI not Available" class="ai ai-doi btn z-depth-0 disabled" role="button"></a>
             -->
          
            <a href="https://drive.google.com/file/d/1KEPs1_EHN9Dy10lAbmdbujojWGa9cPf_/view?usp=sharing" title="Download PDF" class="fa fa-download btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"></a>
          <!--<a href="" title="Code not Available" class="fab fa-github btn z-depth-0 disabled" role="button"></a> -->
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Salient feature extraction in images and videos are of high concern from the aspect of object detection.
Today there are many techniques which are used to extract the salient features. Salient features are basically the
most attention taking features seen by the human eye. In the frequency domain the most appropriate method is the
spectral residual approach using the Phase Fourier Transform (PFT) which gives better result than other techniques.
In this paper we are implementing the spectral residual method using the Phase Fourier Transform to find out the
salient areas. These results have been immensely improved by applying edge detection techniques and
morphological operations. To make the object detectable sobel operator and dilation is used. After applying we get
better results and a very clean view of the salient areas and in some cases we almost prove total object detection.
Furthermore PFT is implemented on videos and for object detection sobel operator and dilation is applied on the
results given by PFT. Finally Area under the Receiver Operating Characteristics (AUC) is calculated for both
images and videos.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azam2015saliency</span><span class="p">,</span>
  <span class="na">show_bib</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Saliency Based Object Detection and Enhancements Using Spectral Residual Approach in Static Images and Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azam, Muhammad Shoaib and Gilani, Syed Omer and Jamil, Mohsin and Ayaz, Yasar and Naveed, Muhammad and Khan, Muhammad Nasir}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advanced Science Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3677--3679}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Scientific Publishers}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>
  </div>


  </article>

  
  <div class="social">
    <div class="contact-icons">
      <a href="mailto:%73%68%6F%61%69%62%61%7A%61@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-3521-5098" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=7twUsBMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/azamshoaib" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/azamshoaib" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            

    </div>
    <div class="contact-note"></div>
  </div>
  </div>

</div>
      
    

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Shoaib  Azam. <br> Built by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and based on the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted on <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: March 16, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?fb357a1858c9874966432bedd22c1c2e"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
